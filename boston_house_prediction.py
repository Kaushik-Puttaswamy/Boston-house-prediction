# -*- coding: utf-8 -*-
"""Boston-house-prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-sxT3na69H_FhOpEkdSEDM5ooF7Ruxn0
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

"""Lets load the Boston house pricing dataset"""

data_url = "http://lib.stat.cmu.edu/datasets/boston"
raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)
data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
target = raw_df.values[1::2, 2]

# You can now use 'data' for features and 'target' for the target variable.
# For example, you could create a pandas DataFrame:
feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']
boston_df = pd.DataFrame(data, columns=feature_names)
boston_df['PRICE'] = target

display(boston_df.head())

boston_df.keys()

"""Preparing the dataset"""

dataset = pd.DataFrame(data, columns=feature_names)

dataset

dataset['Price'] = boston_df['PRICE']

dataset.head()

dataset.info()

#Summarizing the stats of the data
dataset.describe()

#Check the missing values

dataset.isnull().sum()

# EDA
# Correlation
dataset.corr()

sns.pairplot(dataset)

plt.scatter(dataset['CRIM'], dataset['Price'])
plt.xlabel("Crime Rate")
plt.ylabel("Price")

plt.scatter(dataset['RM'], dataset['Price'])
plt.xlabel(" Average No of Rooms")
plt.ylabel("Price")

sns.regplot(x="RM", y="Price", data=dataset)

sns.regplot(x="LSTAT", y="Price", data=dataset)

sns.regplot(x="CHAS", y="Price", data=dataset)

sns.regplot(x="PTRATIO", y="Price", data=dataset)

X= dataset.iloc[:,:-1]
y= dataset.iloc[:,-1]

X.head()

y.head()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)

X_test = scaler.transform(X_test)

X_train

X_test

from sklearn.linear_model import LinearRegression

regression = LinearRegression()

regression.fit(X_train, y_train)

print(regression.coef_)

print(regression.intercept_)

regression.get_params()

#Prediction with test data

regression_pred = regression.predict(X_test)

regression_pred

# plot a scatter plot for the prediction
plt.scatter(y_test, regression_pred)

residuals = y_test - regression_pred

residuals

# Plot this residuals

sns.displot(residuals, kind="kde")

##Scatter plot WRT prediction and residual

plt.scatter(regression_pred, residuals)

from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error

print(mean_absolute_error(y_test, regression_pred))
print(mean_squared_error(y_test, regression_pred))
print(np.sqrt(mean_squared_error(y_test, regression_pred)))

# R-square and adjusted R-square

from sklearn.metrics import r2_score
score = r2_score(y_test, regression_pred)
print(score)

1-(1-score)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)

data[0].reshape(1,-1)

scaler.transform(data[0].reshape(1,-1))

regression.predict(data[0].reshape(1,-1))

regression.predict(scaler.transform(data[0].reshape(1,-1)))

"""Pickling the model file for deployment"""

import pickle

pickle.dump(regression, open('regmodel.pkl', 'wb'))

pickled_model = pickle.load(open('regmodel.pkl', 'rb'))

pickled_model.predict(scaler.transform(data[0].reshape(1,-1)))

